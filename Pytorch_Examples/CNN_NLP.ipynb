{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#                input channel , output channel, kernel size= 3x3 \n",
    "conv = nn.Conv2d(3, 16, kernel_size=3) # kernel_size = (3,3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output pixel is generated by 3x3 matrix. But there are 3 channels. So,\n",
    "each output pixel is generated by 3x3x3 matrix. That is these many weights or parameters need to be determined.\n",
    "\n",
    "But, the output is 16 channels. So, again we will have 16x3x3x3 weights now.\n",
    "Remember, these weights are translational invariant.\n",
    "\n",
    "Plus, there is a bias for channel output, that is, 16 bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "data_path = '/Users/ardhendusingh/Documents/Stanford/Pytorch_Examples/img_data/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False)       \n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False)\n",
    "\n",
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifar10).__mro__\n",
    "len(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32 at 0x7FE86959EA90>, 6)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfMUlEQVR4nO2dbWyc13Xn/2feOMN3UiIpiZItW36pncZWHNXwOtlu0qCFGxR1AiyyyYfAH4KqKBqgAbofjCywyQL7IVlsEuTDIgtl49ZdZPOyeWmMwtg2NVIYbQrXcuz4vbYsy5EoiqJEjsjhDOf17IcZb2Xv/V/SEjlUcv8/QNDwHt7nOXNnzvPM3D/POebuEEL86pPZaQeEEP1BwS5EIijYhUgEBbsQiaBgFyIRFOxCJELuaiab2X0AvgogC+B/uPsXYr+fz+d9oFgM2trtNp2XQVgezBo/VyHHr2P5iC2XzVKbWfiEZpFrZsTHVos/55ggmo35SKTUjnf4uTr8bJaJPIEInU74ucV8jx4v4r9FFpnZMhE/shn+erL3AAB0IjK2x94IbE70eGGWyquoVNeDJ7viYDezLID/BuC3AZwB8KSZPeLuL7I5A8UiDt/13qCtXF6i5xrIhF/oyQJfjOt2DVLb1OQQte0eH6a2QjYfHM8NlOgcZPkSLy2Xqa3R4s9tYnyM2jLtZnC8Xq/TOevr69RWLIUvzgDQBr9YVWuV4PjY+CidA+fHa9Qb1JZF+HUB+MVlZJi/zkND/P2Rz/P1qEV89NgNIRN+j8Sec8vDF48vfuP7/DTcgw25G8AJdz/p7g0A3wZw/1UcTwixjVxNsM8COH3Zz2d6Y0KIa5Cr+s6+GczsKICjADAwMLDdpxNCEK7mzj4H4MBlP+/vjb0Fdz/m7kfc/Uguz79bCSG2l6sJ9icB3GxmN5hZAcDHATyyNW4JIbaaK/4Y7+4tM/s0gL9GV3p7yN1fiM1ZX1/HCy+Gf6V84QKdN0k2QG0X3xnd3R6hNitNU9tah6sClXZ4h9ytQOdU1/mOarXGd8ibbS41XYhojsVc2MdWix8vS3aDgfhXr+r6GrW1OuHnbeu76JxMRJVrRtSEUo6/DypkR3up3aJzBgf5brxl+KdTI2oNACAi51XXwwpKqxkeB4BsLvy6NNdrdM5VfWd390cBPHo1xxBC9Af9BZ0QiaBgFyIRFOxCJIKCXYhEULALkQjb/hd0l5MBUMoR2Sjyx3XXE4nt4AxPCJmemqS2UkxaiWQ11erhhJH1JpeFPHK8QimSQBNJhPEOP9/YZDgBqNXkxyvkuR+RZERkC/xFqzfCa9Vs8fUYjBwvN8R9LEbmtSwsD2YiWXStSIZaLNNyeIgnX1XWqtTWbIUltljC4erKpeB4J5o9KoRIAgW7EImgYBciERTsQiSCgl2IROjrbryZo2jhBISREe7KLbMTwfFdJZ45ke/wUkuVJZ6c0u7w61+tGvY9w/NgMBopc5WL7CKXL63yeZFXbXIkvCO8usKTVhqRhJYaSdIA4nXVhklpp2aDJ2pk2vyJ5SMJOW1SigsAcmT7vF7ncwp5/oJmOjyBpl5ZpjaQJCoAGCBv41aHKwaX1sKKTDtST1B3diESQcEuRCIo2IVIBAW7EImgYBciERTsQiRCX6W3nBkmBsKnLEWklTGSBDE1ymt+tUn7IQCRPiZANhcphEbqiNU7EeknopPlIskY7TqXqDzLr9Hnz5fDx2vyZ71a5Uka1TaXKYdLke4uddL+Cfw5Z4zLRtmBSCeWNS6zDubDPuYirZXWI3UDa00uvXUiTbvKFe5juRp+/1SI1AsA683we6ARqTWoO7sQiaBgFyIRFOxCJIKCXYhEULALkQgKdiES4aqkNzM7BWAVXTWr5e5HoifLGqbGwxLKSJ5LXsVi2JbJcqmjFKnv1mxxGaoTyeTqtqH//2lE6sW1G1yW63gkoywieXmOZ2WtNsIZbO02X99qpNVUK2JbXeP+zy2F/chn+PFGK3ztm+d4e7DaJS4dXrf7puD49PR+OsdGwvXdAKC+fJHaKhWePXhplUtvFy6FZdZTp7kf7Ww4dOsNLtdthc7+QXfnr4QQ4ppAH+OFSISrDXYH8Ddm9pSZHd0Kh4QQ28PVfox/v7vPmdk0gB+b2cvu/vjlv9C7CBwFgGLke7kQYnu5qju7u8/1/j8P4IcA7g78zjF3P+LuRwo5fWsQYqe44ugzsyEzG3nzMYDfAfD8VjkmhNharuZj/AyAH/baJeUA/C93/z+xCflcFvumwoUIRwtcMhgeDEtNFpGuEMlAski2Wb3GZZwMkeV2jfA2VENDPFtr5RIXMcZGeUbZaqQI5Btz4WNW6vwrVIEvB2YHI1l7eZ6Zd+piOThe90iR0EjW29joCLXdeztXfFfmwzKrVyPn2s2zKetVvh6VCr93DuT5MQ/sCT+36ekZOmdhJSzlXXzlHJ1zxcHu7icB3Hml84UQ/UVfooVIBAW7EImgYBciERTsQiSCgl2IROhvwcmsYXIknI2Wa5TpvIF82M3BgXBfMwCo17g81Yz06xofD/eVAwAnRQobbX7NbDYjxRCHeR+4s4vhXl4A8NobPBtqcTX83CK1C3F9pGfeR/71YWrbv5f7/72nTgbH//EEl4ZaHZ7pl8twqWy1vEht1Up4HUdGuBSGNs++Kxb5vALJzgSAQePzWu3wi3PdgX10zshSuBfgs6/ztdCdXYhEULALkQgKdiESQcEuRCIo2IVIhP7uxudymJ7cFbTVlviudcbCblZI2xwAqMVqcVmkHlukTRK7MtaafBd5fIIntDTafIf55Jmz1La0wn1k9emykZZRo0V+vOlceNcXAIpLXDG4eXRPcHx+kvuxUD5PbfUqX+OnX3mF2jKkHVJzKNK6aownoCDDQ2ZsjKtDI51IuylSp9AbK3TOQZJQNpDn66s7uxCJoGAXIhEU7EIkgoJdiERQsAuRCAp2IRKhz9JbHhO7p4K2iWHerimTCScRlFeW6ZzmWoUfrx1r/8QLsjlJyBke5nXmmuC2l05yyWitzlsJFYsD3FYI+1ga4rLQRJbLlE+dWKC2VoO/fepjYeltaoKvh4HLYc0Wl2arDV4Lb43Ummu0+HO2iJQa6Q6GfCbSOiwTqb2XC69jq86lTSeyLcnVAqA7uxDJoGAXIhEU7EIkgoJdiERQsAuRCAp2IRJhQ+nNzB4C8HsAzrv7r/fGJgF8B8BBAKcAfMzduQ72L0cDiIxmkfY4jIFIPbBBhLOCACAXucZlMpF6ckSWGyjx9k8XzvGsseoFvmQ3TnKJqs5VKBSJxHbroVk6JxM5YCvL13glIn3msuE6eSMF/rrsmjhEbYduvo7aXv/Fk9T28itzwfFCLiJrOZdtWy0eMhmScQgA+QJfx04n/L7qRHQ+s/D7NKIMburO/ucA7nvb2IMAHnP3mwE81vtZCHENs2Gw9/qtL71t+H4AD/cePwzgI1vrlhBiq7nS7+wz7j7fe3wO3Y6uQohrmKveoPNuMXX6R3pmdtTMjpvZ8dVq5MumEGJbudJgXzCzvQDQ+5/WE3L3Y+5+xN2PjAzyTSchxPZypcH+CIAHeo8fAPCjrXFHCLFdbEZ6+xaADwDYbWZnAHwOwBcAfNfMPgXgDQAf28zJOu6orYeL61mTZy4B4QyltTVekK/R5NexVoZ/wqhUuVS2QmyzB/gyeosf7/rdXCg5tI9LNdV1Pm/2ljuD4wXnX6GWL/HCnaXxcIFQAMBFnsl1YM/e4Hh5jWfz3fhrN1Pb6ATP2huduI3alhfD6798ibfQykfkwYzzjMNmJ5JNyZMp0W6G39+RJDraiiyS9LZxsLv7J4jpQxvNFUJcO+gv6IRIBAW7EImgYBciERTsQiSCgl2IROhrwUmHo21hecLbvAAgkxlKRV6kcniESzVnF7nM9/qZRWrL5cN+FBZ4X7b1BX68m6e5vPahD3AZ6rW5t6cq/Asjs+GCnrt3hQtAAsD5RV5Ucnw8IkN1uP8FUmDx/GI4Cw0AcsUytS2W56ltbp5nqeXz4ffB+CjXwmo1LmB5jt8fLaKVdSKyXMbC8yySgRlpE8jP886nCCF+GVGwC5EICnYhEkHBLkQiKNiFSAQFuxCJ0FfpLZvNYHx8OGhr5bj0VqmEM7a8yeWMS6s8q+mNX3CpqVLhMk6pGL42zr/Os+9mirwI4ezs9dQ2vu8GasuvRlKoSBHO/Xfezaec43JYqcWlwzZ4Jt3aWti2dzAsDQJAo82flw2F3zcAsH9oH7WNjIclx9WL5+ic8wsXqa1pXG5cb/AilshwrWxoIJyF2ahFJEVSwNKIjAfozi5EMijYhUgEBbsQiaBgFyIRFOxCJEJfd+M77RZWy+GdzlyD12rLk1Y34CXQkMtyY7XCd+onRnjix/hQeNe0tsx346f38Rpus3f8G2p7/kyD2l45wW337p0MjpfLfM7MoXDdOgDIoEptjTrfqR/38M76ynm+011q8Fp4eyfDzwsAym1eFy5/x0RwvBZJrPmHRx+htjOn+XPORlo8xRozsbybZqxNWTO8VixpDNCdXYhkULALkQgKdiESQcEuRCIo2IVIBAW7EImwmfZPDwH4PQDn3f3Xe2OfB/AHAN7UIT7r7o9u5oRZokC0I3/070S2yJC2UADQNi69LXOFBysrkfpj9bB8tXeMy3W/8cEPUtv+W++hth/82UPUtieSFJJthOvrzZ18jR/vxtuprbjrJmobci6XVpfCvT5LnbAUBgCNGpf5Lqxy2/gUTxratedgcLxWGaVzMtyEdoEn/8Rq0DWbXPq0Vjihy5wnerVa4dC9WuntzwHcFxj/irsf7v3bVKALIXaODYPd3R8HwMuZCiF+Kbia7+yfNrNnzewhM+OfzYQQ1wRXGuxfA3AIwGEA8wC+xH7RzI6a2XEzO16p8u8tQojt5YqC3d0X3L3t7h0AXwdAy6C4+zF3P+LuR4YHedUWIcT2ckXBbmZ7L/vxowCe3xp3hBDbxWakt28B+ACA3WZ2BsDnAHzAzA4DcACnAPzhZk5mAIwoA22SxQPwNjiRTjzwWuR4kRJuk7t426g9g2Gp764jt9A5t93L5bXl81xuHGjxzLwb9++ntg55cnumee231jqXMKuRbLlGi89r1sJvrTa4bPja3Blqe+7549R27z3cx117wlmHK6thaRAASMcoAMDug1xm7cTaNTUiMhqRdC8tlumc+mrYyQ7JNgQ2Eezu/onA8Dc2mieEuLbQX9AJkQgKdiESQcEuRCIo2IVIBAW7EInQ14KT7kCHZPjU6lwyKJAsr1yOF/jLZrgcc9Me/te9xRK//h28/kBw/M7388y2vbfeQW3P/OOfUdt1B7iPe971bmorTB0KjucGx+ic6jqXAGsrPLNt4expalteCMto7SbPXiuNhAt6AsDu3fy1Pn32aWqb2TsbHG9VI1mWNd7GydaWqa3t4YxDAHCmOQMoDYSfW2EPf84rAyQTNBLRurMLkQgKdiESQcEuRCIo2IVIBAW7EImgYBciEfoqvZkZ8tnwKZcjBQXb62GZoTRYonOyGS51TEcy207Pl6nt0F2hUnzA/neHx7twCa25ukZtYyNcKpu65TC1reXCPdFeePpJOqde436srJSp7cLcL6gt2w5Ln8Uif8vN3hCWyQDgjlt44ctWlmei5bPj4fECz4rMrfOiktU35qiNycoA0IrcViukL+HgLv68ZkgPwXw+0h+OuyCE+FVCwS5EIijYhUgEBbsQiaBgFyIR+psI0+mgXgvvdA4OcFesGN6tzGd4DTRvc1tpmLeG+v1/9/vUdu/vfig4Prp7hs5ZOPkStWUj/pdXeQ26xVP/TG1nV8M7wn/3l39J5wyXeMLFep0njOyZ4YrB6Eh4J/n1Mzx5phFZj8l9B6ntlne/l9rQHggOL5V5vbsqUX8AYLnGfTTn7+H1Gk/0qpCWTV7hqsBt4+HxDhehdGcXIhUU7EIkgoJdiERQsAuRCAp2IRJBwS5EImym/dMBAH8BYAbddk/H3P2rZjYJ4DsADqLbAupj7s4LdAFwODpOasN1eBKBtcKyRcsjLZ4iNb+KA6PUdvi9XMYZyIclqhef4TXQls++Rm31OpdWVpeXqO30iRepreLh5KB8m59rOMelyNEiT8aYmuDS2/zCueB4K9Lmq7rKZb7Tr/OkG+AFaqlUwjX0ijn+/mgNTFPbxRZ/75RKvIbe4AhP2irlwvLganWFzml1whJgRHnb1J29BeBP3f12APcA+GMzux3AgwAec/ebATzW+1kIcY2yYbC7+7y7/6z3eBXASwBmAdwP4OHerz0M4CPb5KMQYgt4R9/ZzewggPcAeALAjLvP90zn0P2YL4S4Rtl0sJvZMIDvA/iMu7/ly4S7O8jXBTM7ambHzez4Wo3XchdCbC+bCnYzy6Mb6N909x/0hhfMbG/PvhdAsOG1ux9z9yPufmSoVNgKn4UQV8CGwW5mhm4/9pfc/cuXmR4B8EDv8QMAfrT17gkhtorNZL29D8AnATxnZs/0xj4L4AsAvmtmnwLwBoCPbXwoBxCW0Tot/hE/lw/XjGtHan41wLOTZsZ4Xbi/fuSvqG1yJizxTO8Nt4UCgEaVZ6/l82HJBQCGh7jEk8twqWyIyIN7psM1ywCgtsoV01KW+3hx8QK1NRvh12akyCWoRoVLb68+fZza5l9+hdrqLdKSKc/XsB1b3/1cisQQfw9nBrj0WSQy2gT4Wt32rhuC46XiSTpnw2B3978HwHL+wjmfQohrDv0FnRCJoGAXIhEU7EIkgoJdiERQsAuRCH0tOAk3dDrhjf1CJPOqmCPF+jK8MKBHWgJ1Gjzz6sKFcLYWAFQWw7ZSk2cndcCf1+QEl8PG901RW6tdp7a5s2EfPZIPlcnwt0GjxSXMrPFClUPFsFxKEhi7x4sZI1mM7QaXNzPk/bZS5XJjY4DIdQBG9vG1XyuVqW21w2W59bXwPXfX6I10zm4ipeby/LXUnV2IRFCwC5EICnYhEkHBLkQiKNiFSAQFuxCJ0F/pDYaMhbOoigM8w8dJBttQKSzvAMDQyG5qqzZ5BtKuEZ5znyN+NC4t0DmdDD9eNc+lppmZcFYTAHQaXMa59Y79wfGf/uQxOqfhVWrLG5c3axU+b3QknLVXyPG3XNYi/dDW+Wv2+jyX0crl8GtWtzU6Z+oWfg+cHY9k7Tl/rZcv8LUqrIclzKHZSKZiNZxV2Imol7qzC5EICnYhEkHBLkQiKNiFSAQFuxCJ0Nfd+IwBhVz4+lKt8wSDLGlB1InUR6s2eTJDNs+TKgYKfLc1nw/7URjkbZDGRnlCzrlFvotfnQ3vqgPA9IGbqG3ufLgu3Lt+4310TmXxLLWdfIW3VlqrlKktlw2v/9gYr61npD4hAMzPcR9/8UYkEWYgvP6jM1zJmZqM+BhRBWyJv9YTyzzUZqcng+P7x/l74MSL4YSneo0neenOLkQiKNiFSAQFuxCJoGAXIhEU7EIkgoJdiETYUHozswMA/gLdlswO4Ji7f9XMPg/gDwAs9n71s+7+aPRkOcPMVPj60rx4kc6rtcOSzBrPZYBneGuoXCQZY3SUJx8USGul2hqvQVeK1ARDg9uO//Sn1HbjrVyyO3MmLMlkIvX6Bgd4LblsRN4slbjUtFYJS2+1GpdEW5EWYMMl7se977mF2ookIaeV5bX12k2etFI7zaW3zGqR2qYHR6jtPbe8KzxnnHdBf2r+9eB4q8mf12Z09haAP3X3n5nZCICnzOzHPdtX3P2/buIYQogdZjO93uYBzPcer5rZSwBmt9sxIcTW8o6+s5vZQQDvAfBEb+jTZvasmT1kZrw1qhBix9l0sJvZMIDvA/iMu68A+BqAQwAOo3vn/xKZd9TMjpvZ8ZUq/04mhNheNhXsZpZHN9C/6e4/AAB3X3D3trt3AHwdwN2hue5+zN2PuPuR0UFeyUMIsb1sGOxmZgC+AeAld//yZeN7L/u1jwJ4fuvdE0JsFZvZjX8fgE8CeM7MnumNfRbAJ8zsMLpy3CkAf7jRgQoFw3UHwnf3MeOyxYnTYSlkYZFnrzXaXKoZHuZPe63KM6janUpwPBu5Zi4tcklxtcJlkvUm9yPr3DYyHN46WTi3ROecWeNyUse5ZDczxWVK64Szr5bLvF7cwBB/zcbHuHRVyPL1rzeIBJvjcuNanR+vUYm0vOrweTcd2ENt+/aE1/H0GS6xXlwMx0Qr0kJrM7vxfw8g9IpHNXUhxLWF/oJOiERQsAuRCAp2IRJBwS5EIijYhUiEvhaczOYMoxMkc4xICQAwMZ0NG4Z40cALC7yA5XqkfVKuwIsNsmmdJs+wa7a5H5dqXIYaimR5rVe5VFZbDxecbER8bEds7mTtAVRWIu2fRsOFO0dHeXHOWo0f78JFvlbDwzz7zjLh+5m1uGxbyPGiowNcIUahwNfq4E0Hqa1WDfvy+OMv0jnPvnI+fKx1Lufqzi5EIijYhUgEBbsQiaBgFyIRFOxCJIKCXYhE6Kv0ZmbIFcOnLI7yXPfJ4fA1KVfjsla+xLN/ViJ9t9Dm179ScTo8Jc/P1a6Xqa0wyP3I5/h6ZLNccqx72JdGk8uNHslsM65QwRtcAmwTUz6SbYYClxvLy1x6qzV4f7Ox8bCUmiOSHABkImtfBZe2Fi6sUttyJMNxdS2cxfi3f/cyPxdRKdcbkt6ESB4FuxCJoGAXIhEU7EIkgoJdiERQsAuRCH2V3jodQ4UV7MsO03nDQ2EdJ1/iutBQJD1pbIxLZZUV3ousshIuAFipRrLe1rltpMALNhZJXzkAaNW55JjLha/fhchlPT/As7XM+MTBSOHODDG12lwaKpQiPfjGudy4tMQlr1UiRY5O8rWvRnrOvXqKFxB9+bnT1DYzybMpZ/aT55bh79PdpADnwiqXIXVnFyIRFOxCJIKCXYhEULALkQgKdiESYcPdeDMrAngcwEDv97/n7p8zsxsAfBvALgBPAfiku0fbtDYawJk3wrZ6me+ej0yFd3CLpUgCBN/cx+Qkf9qVNV4HrVwO25Yv8sSJZb55i2yH74J3nCsN7Tbf4UcnbItd1S3DE2GyOb5WtUjSkJNN9zxpCwUArSpvUdWO1KdrR5JrypXwPNYVCgCWIorMqRP8BS1fXKO2xho/4Z6xcGuo266fpXOYi6+eW6FzNnNnrwP4LXe/E932zPeZ2T0AvgjgK+5+E4BlAJ/axLGEEDvEhsHuXd7saJjv/XMAvwXge73xhwF8ZDscFEJsDZvtz57tdXA9D+DHAF4DUHb/fx/WzgDgnzmEEDvOpoLd3dvufhjAfgB3A/i1zZ7AzI6a2XEzO36pwosdCCG2l3e0G+/uZQA/AfCvAIyb2Zu7N/sBzJE5x9z9iLsfGRuOVNgXQmwrGwa7mU2Z2XjvcQnAbwN4Cd2g/7e9X3sAwI+2yUchxBawmUSYvQAeNrMsuheH77r7X5nZiwC+bWb/GcDTAL6x0YHccmjndwdtzcIROq/eCSd+ZFrhVkcAUBzjctL4FP+EMZHhiRqT1XBiQnmJtwsqX+DyWm2NL3+7xeU8OL9Gd1phH9dr/CtUoRCpd5fj/q+u80SNGvnKlo+osyOZcHIHAHQyXFJqNvk6DgyFJcxinte7Gy9wH2/EOLW9+07ehurWO+6ktoM33RQcv/seLjeeOVsJjv/DazwmNgx2d38WwHsC4yfR/f4uhPglQH9BJ0QiKNiFSAQFuxCJoGAXIhEU7EIkgnkku2rLT2a2CODNvLfdALhO0D/kx1uRH2/ll82P6919KmToa7C/5cRmx92di+vyQ37Ijy31Qx/jhUgEBbsQibCTwX5sB899OfLjrciPt/Ir48eOfWcXQvQXfYwXIhF2JNjN7D4z+2czO2FmD+6EDz0/TpnZc2b2jJkd7+N5HzKz82b2/GVjk2b2YzN7tff/xA758Xkzm+utyTNm9uE++HHAzH5iZi+a2Qtm9ie98b6uScSPvq6JmRXN7J/M7Oc9P/5Tb/wGM3uiFzffMbNIamQAd+/rPwBZdMta3QigAODnAG7vtx89X04B2L0D5/1NAHcBeP6ysf8C4MHe4wcBfHGH/Pg8gH/f5/XYC+Cu3uMRAK8AuL3faxLxo69rAsAADPce5wE8AeAeAN8F8PHe+H8H8Efv5Lg7cWe/G8AJdz/p3dLT3wZw/w74sWO4++MA3l43+X50C3cCfSrgSfzoO+4+7+4/6z1eRbc4yiz6vCYRP/qKd9nyIq87EeyzAC5vd7mTxSodwN+Y2VNmdnSHfHiTGXef7z0+B2BmB335tJk92/uYv+1fJy7HzA6iWz/hCezgmrzND6DPa7IdRV5T36B7v7vfBeB3Afyxmf3mTjsEdK/s6F6IdoKvATiEbo+AeQBf6teJzWwYwPcBfMbd31Kapp9rEvCj72viV1HklbETwT4H4MBlP9NilduNu8/1/j8P4IfY2co7C2a2FwB6/5/fCSfcfaH3RusA+Dr6tCZmlkc3wL7p7j/oDfd9TUJ+7NSa9M5dxjss8srYiWB/EsDNvZ3FAoCPA3ik306Y2ZCZjbz5GMDvAHg+PmtbeQTdwp3ADhbwfDO4enwUfVgTMzN0axi+5O5fvszU1zVhfvR7TbatyGu/dhjfttv4YXR3Ol8D8B92yIcb0VUCfg7ghX76AeBb6H4cbKL73etT6PbMewzAqwD+FsDkDvnxPwE8B+BZdINtbx/8eD+6H9GfBfBM79+H+70mET/6uiYA7kC3iOuz6F5Y/uNl79l/AnACwP8GMPBOjqu/oBMiEVLfoBMiGRTsQiSCgl2IRFCwC5EICnYhEkHBLkQiKNiFSAQFuxCJ8H8BKtZZn0JVXMYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = cifar10[0]\n",
    "img, label\n",
    "class_names[label]\n",
    "\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Image to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of converting one image to tensor\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "img_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), torch.float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "img_t, _ = tensor_cifar10[99]\n",
    "type(img_t)\n",
    "img_t.shape, img_t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whereas the values in the original PIL image ranged from 0 to 255 (8 bits per channel), \n",
    "# the ToTensor transform turns the data into a 32-bit floating-point per channel, \n",
    "# scaling the values down from 0.0 to 1.0. Let’s verify that:\n",
    "img_t.min(), img_t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe8673da048>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_t.permute(1, 2, 0))  # Changes the order of the axes from C × H × W to H × W × C\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the images by means and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 50000])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s stack all the tensors returned by the dataset along an extra dimension:\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4915, 0.4823, 0.4468])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2470, 0.2435, 0.2616])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean value of image along first dimenstion. The first dimension has value 3\n",
    "# the view will convert [3, 32, 32, 50000] to [3, N]\n",
    "m_value = imgs.view(3, -1).mean(dim=1)  # mean value\n",
    "m_value\n",
    "std_value = imgs.view(3, -1).std(dim=1) # standard deviation\n",
    "std_value\n",
    "\n",
    "transforms.Normalize(m_value, std_value) # generate a normalization transform tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(m_value, std_value)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe86b34bbe0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQVUlEQVR4nO3dfYxc1XnH8e8TY2MTuzFgMCvbxEAdAa3BRlsEAlJIFUpQWkNpCTRCIFGWVkEBiUhFRK0pSqUS8SKURCQGW3ESgqG82SKoiYuoCFUhLGBsgwl2wBRcY0PA4q0YFp7+ca+VtXvPmd2ZO3fW+/w+kuXZc+be+3Dxb2fmnrnnmLsjIuPfp3pdgIg0Q2EXCUJhFwlCYRcJQmEXCUJhFwlin042NrMzgJuBCcBt7v4vLZ6vcb4gZkyZWNn+xv9+1HAl1Q4/1JJ9732Y/me67bX0PqdMT/cdmOmbNLm6fdp+6W1eeL66/cOdMDTklf9x1u44u5lNAF4Avgi8CjwBnO/uz2W2UdiDuHj+rMr2peu2NFxJtbu/v2+y77FXdib7rv/n9D6P/Yt03wV/nu6bfVR1+2kL09ucflJ1+wvPwvvvVYe9k7fxxwOb3P1Fd/8QWAEs6mB/ItJFnYR9FvDKsJ9fLdtEZAzq6DP7SJjZADDQ7eOISF4nYd8CzBn28+yybTfuvgRYAvrMLtJLnbyNfwKYZ2aHmdkk4DxgVT1liUjd2n5ld/chM7sM+DnF0Nsyd3+2tspkrzZWrrpPSrTPm/3t5DbnDByX7Hv4kVOSfV/KXHHvPzHd9/yr1e1Pb0hvMzdxBX/zi+ltOvrM7u4PAg92sg8RaYa+QScShMIuEoTCLhKEwi4ShMIuEkTbN8K0dTB9qUb2cpf+dbrv3enpvsSNbQBM66tuf2covc3S7yU6doB/VP+NMCKyF1HYRYJQ2EWCUNhFglDYRYLo+v3sIuPJmnXpvtTNKQCPvZTue2ljdfv7uUJ25Dqr6ZVdJAiFXSQIhV0kCIVdJAiFXSQIhV0kCN0IIzLOuOtGGJHQFHaRIBR2kSAUdpEgFHaRIBR2kSA6uuvNzDYD7wAfA0Pu3l9HUSJSvzpucT3N3d+oYT8i0kV6Gy8SRKdhd+AXZvakmQ3UUZCIdEenb+NPdvctZnYwsNrMnnf3R4Y/ofwloF8EIj1W23fjzewa4F13vz7zHH03XqTLav9uvJl92sym7XoMnA6sb3d/ItJdnbyNnwncZ2a79vNTd/+3WqoSkdrpFleRcUa3uIoEp7CLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBKGwiwShsIsEobCLBKGwiwShsIsE0TLsZrbMzLab2fphbQeY2Woz21j+vX93yxSRTo3klf2HwBl7tF0FPOTu84CHyp9FZAxrGfZyvfU392heBCwvHy8Hzqq3LBGpW7uf2We6+9by8WsUK7qKyBjWyZLNALi751ZnNbMBYKDT44hIZ9p9Zd9mZn0A5d/bU0909yXu3u/u/W0eS0Rq0G7YVwEXlo8vBFbWU46IdIu5J9+BF08wuwM4FZgBbAMWA/cDdwGHAi8D57r7nhfxqvaVP5iIdMzdraq9ZdjrpLCLdF8q7PoGnUgQCrtIEAq7SBAKu0gQCrtIEAq7SBAKu0gQCrtIEAq7SBAKu0gQCrtIEAq7SBAdT14he4dFmT7dnxyDXtlFglDYRYJQ2EWCUNhFglDYRYLQ1fhx5luJ9m/+5+XJbQ486eZkX8uJBWWvoVd2kSAUdpEgFHaRIBR2kSAUdpEgFHaRIEay/NMy4MvAdnf/w7LtGuAS4PXyaVe7+4MtD6YVYXrm7kzfOQvTfXc+ne77ypkHJvvswd+2Lkq6opMVYX4InFHRfpO7Lyj/tAy6iPRWy7C7+yPouxUie71OPrNfZmZrzWyZme1fW0Ui0hXthv0W4AhgAbAVuCH1RDMbMLNBMxts81giUoO2wu7u29z9Y3f/BLgVOD7z3CXu3u/u/e0WKSKdayvsZtY37MezgfX1lCMi3dLyrjczuwM4FZhhZq8Ci4FTzWwB4MBm4NLulSijseKBtZXta5b9ILnN2fd+L9n3WOZYf5UZXrt/RnX7WW9kdpixaP6sZN/KdVva22kwLcPu7udXNC/tQi0i0kX6Bp1IEAq7SBAKu0gQCrtIEAq7SBAt73qr9WC6663r2vr/ufw/kl120WnJvkmZXe687eLK9n/4m/RATmqyTICXb7s22ff121ck+1Y+/Fxmr6N3cKYv953xX9daRV4nd72JyDigsIsEobCLBKGwiwShsIsEobCLBKGhtxrk/qPmZvperrmOHP+fd9Od3/j7ZNeRP03fEZcbTnog0X5fZpsPMn13ZPo+yfTNml3dvnRHeps/PSo93AiZ8zjviHTfS5kJOP9rdeZ4o9MPDGroTSQ2hV0kCIVdJAiFXSQIhV0kCF2N30PdBeZuw/iDmo+V891Tjk727fPLdJWnZS5Mf+5nufGEqYn29Hxxtt8xmf2lHZC44g7w9aGZle2L51S3A/CT9AgEnzt5hFWNwulVM78Bq9M3+KToaryIKOwiUSjsIkEo7CJBKOwiQSjsIkG0HHozsznAj4CZFCNTS9z9ZjM7ALiT4l6PzcC57v5Wi32NiaG3MVEE8LeZvvRiTfXLzau2LbtlbkGhobZqkc50OvQ2BFzp7kcDJwBfM7OjgauAh9x9HvBQ+bOIjFEtw+7uW939qfLxO8AGYBawCFhePm05cFaXahSRGozqM7uZzQUWAo8DM919a9n1GsXbfBEZo1qu4rqLmU0F7gGucPe3zX73scDdPfV53MwGgIFOCxWRzozold3MJlIE/XZ3v7ds3mZmfWV/H7C9alt3X+Lu/e7eX0fBItKelmG34iV8KbDB3W8c1rUKuLB8fCGwsv7yRKQuIxl6Oxn4JbCO3033dTXF5/a7gEMpplM7193fbLGvWke9Tsr0PVrngaQZh5yS7jvquEzfoem+/ROXkt7KDCpOyXy6PfPL6b7JqTv9gBmZAc7U4Y6YnN6GnZWtuaG3lp/Z3f1RoHJj4E9abS8iY4O+QScShMIuEoTCLhKEwi4ShMIuEkSjE05OMvPUAMSMzHapBXc2dVhPMzIDHkddmu7LzfSYmyzxpcSEjvdmJi984/50X1ZmyCt5v1z1kNH48Jl01yEnpvuu/LPq9o2ZpaY2vlDZ3D+4ksG3X9eEkyKRKewiQSjsIkEo7CJBKOwiQSjsIkE0OvR2kJkvSvTNyWx3ZKL9Kx3W04h9/ijdN/REc3VICFrrTUQUdpEoFHaRIBR2kSAUdpEgGr0aP93MT0305RYLeqALtYiMFQsS7c+0uT/X1XiR2BR2kSAUdpEgFHaRIBR2kSAUdpEgWq4IY2ZzgB9RLMnswBJ3v9nMrgEuAV4vn3q1uz+Y29fvAamZ1XaMsOBeej/Rvj6zTe4EZxY0knHmvExfu0NsozWSJZuHgCvd/SkzmwY8aWary76b3P367pUnInUZyVpvW4Gt5eN3zGwDMKvbhYlIvUb1md3M5gILKVZwBbjMzNaa2TIz27/u4kSkPiMOu5lNBe4BrnD3t4FbgCMovu23Fbghsd2AmQ2a2WBmFmwR6bIRhd3MJlIE/XZ3vxfA3be5+8fu/glwK3B81bbuvsTd+929P7N6tYh0Wcuwm5kBS4EN7n7jsPa+YU87m/xFaRHpsZFcjT8JuABYZ2ZryrargfPNbAHFcNxmILOWUWHSPjA3sc7T9NdGUEkDKm8XGmOau09R6nJnG9t8deFZyb7586uvkX/nZ3cltxnJ1fhHqc5AdkxdRMYWfYNOJAiFXSQIhV0kCIVdJAiFXSSIRiec/KyZX53oazluV6Plmb6Laj5W7rfpJ23uM3eX1DFt7lM699+Zvs/WfKz9Eu0fAB9rwkmR2BR2kSAUdpEgFHaRIBR2kSAUdpEgRnLXW20m7ANTE3e93Zy56+3ymuu4qOb95bQ7vJZzbKZPd8T1zi0NHis1+WmOXtlFglDYRYJQ2EWCUNhFglDYRYJQ2EWCaHTobeJEOKSvuu/HmaG3axPtb3ZcUT3OyfTlTnA7kxDK2LW15v39cabvg0R7bopnvbKLBKGwiwShsIsEobCLBKGwiwTR8mq8mU0GHgH2LZ9/t7svNrPDgBXAgcCTwAXu/mFuX1P2+xTz50+p7Jv99HvJ7X7eqsgeu+Q7K5J961c9kOy7c/VPaq/lM4n2t2s/knRbbkW0uZOr2yfsTG8zklf2ncAX3P1YiuWZzzCzE4DrgJvc/feBt4CLR7AvEemRlmH3wq6l1SeWfxz4AnB32b4cOKsbBYpIPUa6PvuEcgXX7cBq4DfADncfKp/yKlC9rKSIjAkjCru7f+zuC4DZwPHAkSM9gJkNmNmgmQ3+9gNNrSDSK6O6Gu/uO4CHgROB6Wa26wLfbGBLYpsl7t7v7v0HTt4bVj8XGZ9aht3MDjKz6eXjKcAXgQ0Uof/L8mkXAiu7VKOI1GAkN8L0AcvNbALFL4e73P0BM3sOWGFm3wKeBpa2PNjMgzj4yq9W9l170P3J7dbf8GJl++OtDtiQxdelh94WHNPsgkwaYhs/Xs/0Xbf4/sr2Td+9MrlNy7C7+1pgYUX7ixSf30VkL6Bv0IkEobCLBKGwiwShsIsEobCLBGHuzX2rzcxeB14uf5wBvNHYwdNUx+5Ux+72tjo+6+4HVXU0GvbdDmw26O79PTm46lAdAevQ23iRIBR2kSB6GfYlPTz2cKpjd6pjd+Omjp59ZheRZultvEgQPQm7mZ1hZr82s01mdlUvaijr2Gxm68xsjZkNNnjcZWa23czWD2s7wMxWm9nG8u/9e1THNWa2pTwna8zszAbqmGNmD5vZc2b2rJldXrY3ek4ydTR6Tsxsspn9ysyeKev4p7L9MDN7vMzNnWY2aVQ7dvdG/wATKKa1OhyYBDwDHN10HWUtm4EZPTju54HjgPXD2r4NXFU+vgq4rkd1XAN8o+Hz0QccVz6eBrwAHN30OcnU0eg5AQyYWj6eSHE39wnAXcB5Zfv3gb8bzX578cp+PLDJ3V/0YurpFcCiHtTRM+7+CP9/XcpFFBN3QkMTeCbqaJy7b3X3p8rH71BMjjKLhs9Jpo5GeaH2SV57EfZZwCvDfu7lZJUO/MLMnjSzgR7VsMtMd9+1EOhrwMwe1nKZma0t3+Z3/ePEcGY2l2L+hMfp4TnZow5o+Jx0Y5LX6BfoTnb344AvAV8zs8/3uiAofrNT/CLqhVuAIyjWCNgK3NDUgc1sKnAPcIW77zbpTpPnpKKOxs+JdzDJa0ovwr4FmDPs5+Rkld3m7lvKv7cD99HbmXe2mVkfQPn39l4U4e7byn9onwC30tA5MbOJFAG73d3vLZsbPydVdfTqnJTH3sEoJ3lN6UXYnwDmlVcWJwHnAauaLsLMPm1m03Y9Bk4nv5Z9t62imLgTejiB565wlc6mgXNiZkYxh+EGd79xWFej5yRVR9PnpGuTvDZ1hXGPq41nUlzp/A3wzR7VcDjFSMAzwLNN1gHcQfF28COKz14XU6yZ9xCwEfh34IAe1fFjYB2wliJsfQ3UcTLFW/S1wJryz5lNn5NMHY2eE+AYiklc11L8YvnHYf9mfwVsAv4V2Hc0+9U36ESCiH6BTiQMhV0kCIVdJAiFXSQIhV0kCIVdJAiFXSQIhV0kiP8D60grOEiYJvEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_t, _ = transformed_cifar10[99]\n",
    " \n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Cifar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in transformed_cifar10\n",
    "          if label in [0, 2]]\n",
    "# cifar2_val = [(img, label_map[label])\n",
    "#               for img, label in cifar10_val\n",
    "#               if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = cifar2[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the convolutional translation. We will get the output of shape 16 channels\n",
    "\n",
    "#                input channel , output channel, kernel size= 3x3 \n",
    "conv = nn.Conv2d(3, 16, kernel_size=3) # kernel_size = (3,3)\n",
    "conv\n",
    "\n",
    "#Size of the image will drop from 32,32 to 30x30. Edges or boundary pixel lost. No padding\n",
    "\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we are going to display the 'output', but only the first channel out of 16 channels\n",
    "\n",
    "Size of the image dropped from 32,32 to 30x30. \n",
    "Edges or boundary pixel lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 30, 30])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30, 30])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 30])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output.shape\n",
    "output[0].shape\n",
    "output[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe867547f98>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0661,  0.0500,  0.0344],\n",
       "          [ 0.0997,  0.1808,  0.1572],\n",
       "          [-0.0427, -0.0397, -0.0702]],\n",
       "\n",
       "         [[-0.1790,  0.1837,  0.1647],\n",
       "          [-0.0789,  0.0786,  0.0435],\n",
       "          [ 0.0256,  0.1623,  0.0511]],\n",
       "\n",
       "         [[-0.1841, -0.0886,  0.1292],\n",
       "          [-0.1804, -0.0700, -0.0147],\n",
       "          [-0.1064, -0.0269,  0.1169]]]], requires_grad=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill kernel weights by hands\n",
    "\n",
    "- Let’s first zero out bias, just to remove any confounding factors, and \n",
    "- then set weights to a constant value so that each pixel in the output gets the mean of its neighbors. \n",
    "\n",
    "- For each 3 × 3 neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111]],\n",
       "\n",
       "         [[0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111]],\n",
       "\n",
       "         [[0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111]]]], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111]],\n",
       "\n",
       "         [[0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111]],\n",
       "\n",
       "         [[0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111],\n",
       "          [0.1111, 0.1111, 0.1111]]]], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0/9.0)\n",
    "\n",
    "conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe8601ca358>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge detection kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working out the weighted sum for an arbitrary pixel in position 2,2, as we did earlier for the generic convolution kernel, we get\n",
    "\n",
    "\n",
    "o22 = $$\\begin{pmatrix}i13 - i11 +\\\\\\\n",
    "      i23 - i21 +\\\\\\\n",
    "      i33 - i31\\end{pmatrix}$$ \n",
    "      \n",
    "      \n",
    "which performs the difference of all pixels on the right of i22 minus the pixels on the left of i22. If the kernel is applied on a vertical boundary between two adjacent regions of different intensity, o22 will have a high value. If the kernel is applied on a region of uniform intensity, o22 will be zero. It’s an edge-detection kernel: the kernel highlights the vertical edge between two horizontally adjacent regions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe85d88e898>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()\n",
    "    \n",
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.conv.Conv2d"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.pooling.MaxPool2d"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(conv)\n",
    "type(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    " \n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example Conv 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # ...               \n",
    "            nn.Linear(8 * 8 * 8, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2))\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [64 x 8], m2: [512 x 32] at /Users/distiller/project/conda/conda-bld/pytorch_1579022036889/work/aten/src/TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-9c784fd7714c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Applications/anaconda3/envs/local_nmt/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/local_nmt/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/local_nmt/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/local_nmt/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/envs/local_nmt/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [64 x 8], m2: [512 x 32] at /Users/distiller/project/conda/conda-bld/pytorch_1579022036889/work/aten/src/TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "## .Not being able to do this kind of operation inside of nn.Sequential was an explicit design \n",
    "## choice by the PyTorch authors and was left that way for a long time; see the linked comments \n",
    "## from @soumith at https://github.com/pytorch/pytorch/issues/2486. \n",
    "## Recently, PyTorch gained an nn.Flatten layer.\n",
    "\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To fix the above problem of reshaping using view function, we have to define a new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## are we saying that \n",
    "        ## Interestingly, assigning an instance of nn.Module to an attribute in an nn.Module, \n",
    "        # as we did in the earlier constructor, automatically registers the module as a submodule.\n",
    "        \n",
    "        ## As a result of which, this allows the class Net to have access to the parameters \n",
    "        # of its submodules without further action by the user:\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            out = self.pool1(self.act1(self.conv1(x)))\n",
    "            out = self.pool2(self.act2(self.conv2(out)))\n",
    "            out = out.view(-1, 8 * 8 * 8)   #  This reshape is what we were missing earlier\n",
    "            out = self.act3(self.fc1(out))\n",
    "            out = self.fc2(out)\n",
    "            return out\n",
    "\n",
    "model = Net()\n",
    " \n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1562, -0.0231]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip connection or ResNet or residual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        ####. Here we are adding output of Relu and out1: skip connection\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building deeper network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        #The BatchNorm layer would cancel the effect of bias, so it is customarily left out.\n",
    "        # so bias = False\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)  \n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        \n",
    "        #Uses custom initializations. kaiming_normal_ initializes with normal random elements \n",
    "        # with standard deviation as computed in the ResNet paper. \n",
    "        # The batch norm is initialized to produce output distributions that initially \n",
    "        # have 0 mean and 0.5 variance.\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,nonlinearity='relu')\n",
    "        \n",
    "        \n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, in init, we create nn.Sequential containing a list of ResBlock instances. \n",
    "# nn.Sequential will ensure that the output of one block is used as input to the next. \n",
    "# It will also ensure that all the parameters in the block are visible to Net. \n",
    "# Then, in forward, we just call the sequential to traverse the 100 blocks and generate the output:\n",
    "\n",
    "\n",
    "# In the implementation, we parameterize the actual number of layers, \n",
    "# which is important for experimentation and reuse. Also, needless to say, \n",
    "# backpropagation will work as expected. Unsurprisingly, \n",
    "# the network is quite a bit slower to converge. \n",
    "# It is also more fragile in convergence. \n",
    "# This is why we used more-detailed initializations and trained our \n",
    "# NetRes with a learning rate of 3e - 3 instead of the 1e - 2 we used for the other networks. \n",
    "# We trained none of the networks to convergence, \n",
    "# but we would not have gotten anywhere without these tweaks.\n",
    "\n",
    "\n",
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        \n",
    "        ## Sequential with list of blocks\n",
    "        self.resblocks = nn.Sequential( *(n_blocks * [ResBlock(n_chans=n_chans1)])   )\n",
    "        \n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a batch data set of 128\n",
    "Each surname is 2D matrix.  Number of rows 77, which are all the unique characters. \n",
    "17 are the number of columns. Maximum number of character in any surname.\n",
    "\n",
    "So, each column has 77 long. Only one of them is set to 1\n",
    "128, 77, 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "        \n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_vocab (Vocabulary): maps characters to integers\n",
    "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
    "            max_surname_length (int): the length of the longest surname\n",
    "        \"\"\"\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self._max_surname_length = max_surname_length\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname (str): the surname\n",
    "        Returns:\n",
    "            one_hot_matrix (np.ndarray): a matrix of one-hot vectors\n",
    "        \"\"\"\n",
    "\n",
    "        #print(f'len(self.surname_vocab) {len(self.surname_vocab)} self._max_surname_length {self._max_surname_length}')\n",
    "        one_hot_matrix_size = (len(self.surname_vocab), self._max_surname_length)\n",
    "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "                               \n",
    "        for position_index, character in enumerate(surname):\n",
    "            character_index = self.surname_vocab.lookup_token(character)\n",
    "            one_hot_matrix[character_index][position_index] = 1\n",
    "        \n",
    "        return one_hot_matrix\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the surnames dataset\n",
    "        Returns:\n",
    "            an instance of the SurnameVectorizer\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        max_surname_length = 0\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            max_surname_length = max(max_surname_length, len(row.surname))\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        print('surname_vocab ',len(surname_vocab))\n",
    "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab, \n",
    "                   max_surname_length=contents['max_surname_length'])\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable(), \n",
    "                'max_surname_length': self._max_surname_length}\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            name_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (SurnameVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "        \n",
    "        # Class weights\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split=='train']\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer. \n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        surname_matrix = \\\n",
    "            self._vectorizer.vectorize(row.surname)\n",
    "\n",
    "        nationality_index = \\\n",
    "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "\n",
    "        return {'x_surname': surname_matrix,\n",
    "                'y_nationality': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Understanding surname data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "surname_df = pd.read_csv(surname_csv)\n",
    "train_surname_df = surname_df[surname_df.split=='train']\n",
    "#return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Totah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Abboud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Fakhoury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Srour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sayegh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nationality  nationality_index  split   surname\n",
       "0      Arabic                 15  train     Totah\n",
       "1      Arabic                 15  train    Abboud\n",
       "2      Arabic                 15  train  Fakhoury\n",
       "3      Arabic                 15  train     Srour\n",
       "4      Arabic                 15  train    Sayegh"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(nationality_vocab) 18\n",
      "len surname_vocab 77\n",
      "len max_surname_length 17\n"
     ]
    }
   ],
   "source": [
    "surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "nationality_vocab = Vocabulary(add_unk=False)\n",
    "max_surname_length = 0\n",
    "\n",
    "for index, row in train_surname_df.iterrows():\n",
    "    max_surname_length = max(max_surname_length, len(row.surname))\n",
    "   \n",
    "    for letter in row.surname:\n",
    "        _= surname_vocab.add_token(letter)\n",
    "    _=nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "print('len(nationality_vocab)', len(nationality_vocab))\n",
    "print('len surname_vocab', len(surname_vocab))\n",
    "print('len max_surname_length', max_surname_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_counts and len 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'English': 2972,\n",
       " 'Russian': 2373,\n",
       " 'Arabic': 1603,\n",
       " 'Japanese': 775,\n",
       " 'Italian': 600,\n",
       " 'German': 576,\n",
       " 'Czech': 414,\n",
       " 'Spanish': 258,\n",
       " 'Dutch': 236,\n",
       " 'French': 229,\n",
       " 'Chinese': 220,\n",
       " 'Irish': 183,\n",
       " 'Greek': 156,\n",
       " 'Polish': 120,\n",
       " 'Korean': 77,\n",
       " 'Scottish': 75,\n",
       " 'Vietnamese': 58,\n",
       " 'Portuguese': 55}"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1603,\n",
       " 220,\n",
       " 414,\n",
       " 236,\n",
       " 2972,\n",
       " 229,\n",
       " 576,\n",
       " 156,\n",
       " 183,\n",
       " 600,\n",
       " 775,\n",
       " 77,\n",
       " 120,\n",
       " 55,\n",
       " 2373,\n",
       " 75,\n",
       " 258,\n",
       " 58]"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0006, 0.0045, 0.0024, 0.0042, 0.0003, 0.0044, 0.0017, 0.0064, 0.0055,\n",
       "        0.0017, 0.0013, 0.0130, 0.0083, 0.0182, 0.0004, 0.0133, 0.0039, 0.0172])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_surname_length\n",
    "# nationality_vocab._token_to_idx\n",
    "# nationality_vocab._idx_to_token\n",
    "\n",
    "class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "print('class_counts and len', len(class_counts))\n",
    "class_counts\n",
    "\n",
    "\n",
    "def sort_key(item):\n",
    "    return nationality_vocab.lookup_token(item[0])\n",
    "\n",
    "sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "\n",
    "frequencies = [count for _, count in sorted_counts]\n",
    "frequencies\n",
    "\n",
    "class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'@'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'T'"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(surname_vocab._token_to_idx)\n",
    "len(surname_vocab._idx_to_token)\n",
    "surname_vocab._idx_to_token[0]\n",
    "surname_vocab._idx_to_token[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 17)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(77, 17)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to vector\n",
    "len(surname_vocab), max_surname_length\n",
    "one_hot_matrix_size = (len(surname_vocab), max_surname_length)\n",
    "one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "one_hot_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n",
      "4 1\n",
      "11 2\n",
      "5 3\n",
      "2 4\n",
      "8 5\n",
      "12 6\n",
      "13 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77, 17)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surname = 'Fakhoury'\n",
    "for position_index, character in enumerate(surname):\n",
    "    character_index = surname_vocab.lookup_token(character)\n",
    "    one_hot_matrix[character_index][position_index] = 1\n",
    "    print(character_index, position_index)\n",
    "\n",
    "one_hot_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]], dtype=float32)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_matrix[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(217, [210, 7])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output_ch, input_cha, kernel_size + bias(number of output channel)\n",
    "\n",
    "a = nn.Conv1d(in_channels=10, out_channels=7, kernel_size=3)\n",
    "\n",
    "7 * 10 * 3 + 7\n",
    "\n",
    "numel_list = [p.numel() for p in a.parameters()]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "# input going as shape torch.Size([128, 77, 17])\n",
    "# 77 channels ( are the unique charaters ever seen in any surname)\n",
    "# 17 lenght of any surname\n",
    "\n",
    "# so, how does any surname looks. Say 'Teflon'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "tensor([[[1., 1., 0.],\n",
      "         [1., 1., 0.],\n",
      "         [1., 1., 0.],\n",
      "         [1., 1., 0.],\n",
      "         [1., 1., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.2114,  0.3161],\n",
       "         [ 0.1838,  0.3037],\n",
       "         [-0.2289, -0.0078],\n",
       "         [ 0.1660,  0.1482],\n",
       "         [-0.2486,  0.0397]],\n",
       "\n",
       "        [[ 0.1582, -0.2616],\n",
       "         [-0.2169,  0.1383],\n",
       "         [-0.1542, -0.0966],\n",
       "         [ 0.0433, -0.0915],\n",
       "         [-0.1539, -0.0565]]], requires_grad=True)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0461, -0.1692], requires_grad=True)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]], requires_grad=True)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.)\n",
      "tensor([[[10.,  5.],\n",
      "         [10.,  5.]]])\n",
      "torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "my_input = torch.ones(1, 3, 5) # 1 is batch, 3 words, each word has 5 dimensions\n",
    "my_input.shape\n",
    "\n",
    "my_input\n",
    "\n",
    "# we want number of dimensions as rows and number of words as columns\n",
    "rand_arr_permute = my_input.clone().permute(0,2,1)\n",
    "\n",
    "rand_arr_permute.shape\n",
    "print(rand_arr_permute)\n",
    "\n",
    "rand_arr_permute[0,:, 2]=0\n",
    "print(rand_arr_permute)\n",
    "\n",
    "conv1 = nn.Conv1d(5, 2, 2)\n",
    "conv1.weight\n",
    "conv1.bias\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv1.weight.fill_(1)\n",
    "    conv1.bias.fill_(0)\n",
    "    print(torch.sum((rand_arr_permute[:,:,:2]*conv1.weight)))# + conv1.bias)\n",
    "    \n",
    "    output = conv1(rand_arr_permute)\n",
    "    print(output)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Understanding surname data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surname_vocab  77\n"
     ]
    }
   ],
   "source": [
    "# create dataset and vectorizer\n",
    "surname_csv = './data/surnames/surnames_with_splits.csv'\n",
    "save_surname_json = './data/surnames/vectorize.json'\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(surname_csv)\n",
    "dataset.save_vectorizer(save_surname_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying to understand following code\n",
    "\n",
    "# dataset.set_split('train')\n",
    "# batch_generator = generate_batches(dataset,batch_size=args.batch_size, device=args.device)\n",
    "\n",
    "## here is how following function is defined\n",
    "\n",
    "#def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "\n",
    "#     dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "#                             shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128,\n",
    "                        shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 77, 17])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['x_surname'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "2\n",
      "dict_keys(['x_surname', 'y_nationality'])\n",
      "torch.Size([128, 77, 17])\n",
      " name is x_surname shape = torch.Size([128, 77, 17])\n",
      " name is y_nationality shape = torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for data_dict in dataloader:\n",
    "    #print(data_dict)\n",
    "    print(type(data_dict))\n",
    "    print(len(data_dict))\n",
    "    print(data_dict.keys())\n",
    "    print(data_dict['x_surname'].shape)\n",
    "    for name, tensor in data_dict.items():\n",
    "        print(f' name is {name} shape = {tensor.shape}')\n",
    "        #pass\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (<ipython-input-291-158885d86d54>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-291-158885d86d54>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    yield out_data_dict\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "for data_dict in dataloader:\n",
    "    out_data_dict = {}\n",
    "    for name, tensor in data_dict.items():\n",
    "        out_data_dict[name] = data_dict[name].to(device)\n",
    "    yield out_data_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    def __init__(self, initial_num_channels, num_classes, num_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_num_channels (int): size of the incoming feature vector\n",
    "            num_classes (int): size of the output prediction vector\n",
    "            num_channels (int): constant channel size to use throughout network\n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=initial_num_channels, \n",
    "                      out_channels=num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x_surname, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_surname (torch.Tensor): an input data tensor. \n",
    "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "        a = self.convnet(x_surname)\n",
    "        print('the convet shape ', a.shape)\n",
    "        features = a.squeeze(dim=2)\n",
    "        #features = self.convnet(x_surname).squeeze(dim=2)\n",
    "       \n",
    "        prediction_vector = self.fc(features)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input channel  77 output channel  18  hidden layer output size  256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(654610, [59136, 256, 196608, 256, 196608, 256, 196608, 256, 4608, 18])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SurnameClassifier(\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv1d(77, 256, kernel_size=(3,), stride=(1,))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,))\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): Conv1d(256, 256, kernel_size=(3,), stride=(2,))\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "    (7): ELU(alpha=1.0)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_channels = 256\n",
    "len_surname_vocab = len(vectorizer.surname_vocab) # this is our input\n",
    "len_nationality = len(vectorizer.nationality_vocab) # this is our output\n",
    "num_of_output_of_first_hidden_layer = 256\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('input channel ', len_surname_vocab, 'output channel ', len_nationality, ' hidden layer output size ',num_of_output_of_first_hidden_layer )\n",
    "classifier = SurnameClassifier(initial_num_channels=len_surname_vocab,\n",
    "                               num_classes=len_nationality,\n",
    "                               num_channels=num_of_output_of_first_hidden_layer)\n",
    "\n",
    "len(list(classifier.parameters()))\n",
    "\n",
    "\n",
    "numel_list = [p.numel() for p in classifier.parameters()]\n",
    "sum(numel_list), numel_list\n",
    "\n",
    "classifier.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SurnameClassifier(\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv1d(77, 256, kernel_size=(3,), stride=(1,))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,))\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): Conv1d(256, 256, kernel_size=(3,), stride=(2,))\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "    (7): ELU(alpha=1.0)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for explanation see above\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "dataset.set_split('train')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=128, \n",
    "                                   device='cpu')\n",
    "\n",
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "dict_keys(['x_surname', 'y_nationality'])\n"
     ]
    }
   ],
   "source": [
    "## total number of batches available. Each batch is of size 128\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    print(batch_index)\n",
    "    print(batch_dict.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_dict size  torch.Size([128, 77, 17])\n",
      "the convet shape  torch.Size([128, 256, 1])\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    optimizer.zero_grad()\n",
    "    print('batch_dict size ', batch_dict['x_surname'].shape)\n",
    "\n",
    "    # step 2. compute the output\n",
    "    y_pred = classifier(batch_dict['x_surname'])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NMT",
   "language": "python",
   "name": "nmt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
